{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scienceplots\n",
    "import shap\n",
    "\n",
    "import events_package.utils as utils\n",
    "from events_package.Experiment import Experiment\n",
    "from events_package.config import FIVE_LAYERS\n",
    "from events_package.input_getters import get_Y_1, get_X_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experiment.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Single Particle Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444840"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing data\n",
    "dataset_df = pd.read_parquet(\n",
    "    r\"C:\\Users\\User1\\Desktop\\MSci_Project\\Data\\6_data\\Electron\\Parquet\\1m_electron_pq_3\"\n",
    ")\n",
    "\n",
    "electrons = Experiment(dataset_df, config=FIVE_LAYERS)\n",
    "del dataset_df\n",
    "electrons.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Removed duplicates\n",
      "INFO: Denoisified the dataset\n",
      "INFO: Shuffled dataset\n",
      "INFO: Number of events after removing duplicates: 434998\n",
      "INFO: Removed events with 0 energy in layers after denoisifying\n",
      "INFO: Number of events after removing 0 energy (in calorimeters) events: 434998\n"
     ]
    }
   ],
   "source": [
    "electrons.standard_procedure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Photons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444142"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_parquet(\n",
    "    r\"C:\\Users\\User1\\Desktop\\MSci_Project\\Data\\6_data\\Photon\\Parquet\\1m_photon_pq\"\n",
    ")\n",
    "\n",
    "photons = Experiment(dataset_df, config=FIVE_LAYERS)\n",
    "del dataset_df\n",
    "photons.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Removed duplicates\n",
      "INFO: Denoisified the dataset\n",
      "INFO: Shuffled dataset\n",
      "INFO: Number of events after removing duplicates: 434870\n",
      "INFO: Removed events with 0 energy in layers after denoisifying\n",
      "INFO: Number of events after removing 0 energy (in calorimeters) events: 434870\n"
     ]
    }
   ],
   "source": [
    "photons.standard_procedure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Neutral Pions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412856"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_parquet(\n",
    "    r\"C:\\Users\\User1\\Desktop\\MSci_Project\\Data\\6_data\\PiZero\\Parquet\\pq_pi0_2\"\n",
    ")\n",
    "\n",
    "pi0 = Experiment(dataset_df, config=FIVE_LAYERS)\n",
    "del dataset_df\n",
    "pi0.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Removed duplicates\n",
      "INFO: Denoisified the dataset\n",
      "INFO: Shuffled dataset\n",
      "INFO: Number of events after removing duplicates: 391483\n",
      "INFO: Removed events with 0 energy in layers after denoisifying\n",
      "INFO: Number of events after removing 0 energy (in calorimeters) events: 391483\n"
     ]
    }
   ],
   "source": [
    "pi0.standard_procedure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Charged Pions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357554"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_parquet(\n",
    "    r\"C:\\Users\\User1\\Desktop\\MSci_Project\\Data\\6_data\\PiPlusMinus\\Parquet\\pq_piplusminus_2\"\n",
    ")\n",
    "\n",
    "pi_char = Experiment(dataset_df, config=FIVE_LAYERS)\n",
    "del dataset_df\n",
    "pi_char.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Removed duplicates\n",
      "INFO: Denoisified the dataset\n",
      "INFO: Shuffled dataset\n",
      "INFO: Number of events after removing duplicates: 330809\n",
      "INFO: Removed events with 0 energy in layers after denoisifying\n",
      "INFO: Number of events after removing 0 energy (in calorimeters) events: 330803\n"
     ]
    }
   ],
   "source": [
    "pi_char.standard_procedure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Combining Different Particle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add types to allow for identification later\n",
    "electrons.add_physics_object_type(typ=\"electron\")\n",
    "photons.add_physics_object_type(typ=\"photon\")\n",
    "pi0.add_physics_object_type(typ=\"pi0\")\n",
    "pi_char.add_physics_object_type(typ=\"pi_char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = electrons + photons + pi0 + pi_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1592154\n"
     ]
    }
   ],
   "source": [
    "# all previous datasets have already been denoisified, duplicates were removed, no need to do it now\n",
    "# in fact, doing it would delete some good events\n",
    "experiment.shuffle_dataset(repeats=11)\n",
    "print(experiment.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318431, 22)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(experiment\u001b[38;5;241m.\u001b[39mX_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m600\u001b[39m,\n\u001b[0;32m     13\u001b[0m }\n\u001b[1;32m---> 16\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_xgboost_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\user1\\documents\\github\\ml-for-atlas-hardware-trigger-at-hl-lhc-in-hh-4b\\events_package\\events_package\\Experiment.py:372\u001b[0m, in \u001b[0;36mExperiment.train_xgboost_model\u001b[1;34m(self, params, normalise)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_test) \u001b[38;5;241m*\u001b[39m u\n\u001b[0;32m    376\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_pred)\n",
      "File \u001b[1;32mc:\\Users\\User1\\anaconda3\\envs\\Environment4\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User1\\anaconda3\\envs\\Environment4\\Lib\\site-packages\\xgboost\\sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m (\n\u001b[0;32m   1017\u001b[0m     model,\n\u001b[0;32m   1018\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1024\u001b[0m )\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User1\\anaconda3\\envs\\Environment4\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User1\\anaconda3\\envs\\Environment4\\Lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User1\\anaconda3\\envs\\Environment4\\Lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# split data into training and testing, next train XGBoost model\n",
    "experiment.train_test_split(get_X=get_X_5, get_Y=get_Y_1, test_size=0.2)\n",
    "print(experiment.X_test.shape)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.18,\n",
    "    # \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"n_estimators\": 600,\n",
    "}\n",
    "\n",
    "\n",
    "experiment.train_xgboost_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.count_nodes(experiment.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Scan in Search of Smaller Model\n",
    "\n",
    "In cells below, tables represent models trained with different hyperparameters: number of rounds (trees), max depth, colsumple by tree and learnign rate.\n",
    "\n",
    "In the end, best model with < 2000 nodes was the one with 130 trees, max depth of 4, colsample of 1.0 and a surprisingly high learning rate of 0.74 (although there was no strong dependency on this parameter, compared to for example no trees or depth). this miniutarised model has 1950 nodes in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_range = [6]\n",
    "learning_rate_range = [0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2]\n",
    "colsample_bytree_range = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "num_rounds_grid = [600]\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(\n",
    "    itertools.product(\n",
    "        max_depth_range, learning_rate_range, colsample_bytree_range, num_rounds_grid\n",
    "    )\n",
    ")\n",
    "\n",
    "# Construct the param_grid\n",
    "param_grid = [\n",
    "    {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"max_depth\": max_depth,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"n_estimators\": n_estimators,\n",
    "    }\n",
    "    for max_depth, learning_rate, colsample_bytree, n_estimators in param_combinations\n",
    "]\n",
    "\n",
    "\n",
    "# Perform hyperparameter scan\n",
    "experiment_hyperparams1 = experiment.xgboost_hyperparameter_scan(\n",
    "    param_grid, nodes_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_hyperparams1.sort_values(by=\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_range = [3, 4, 5, 6]\n",
    "learning_rate_range = [0.18]\n",
    "colsample_bytree_range = [0.8]\n",
    "num_rounds_grid = [130, 150, 200, 300, 400, 500, 600]\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(\n",
    "    itertools.product(\n",
    "        max_depth_range, learning_rate_range, colsample_bytree_range, num_rounds_grid\n",
    "    )\n",
    ")\n",
    "\n",
    "# Construct the param_grid\n",
    "param_grid = [\n",
    "    {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"max_depth\": max_depth,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"n_estimators\": n_estimators,\n",
    "    }\n",
    "    for max_depth, learning_rate, colsample_bytree, n_estimators in param_combinations\n",
    "]\n",
    "\n",
    "\n",
    "# Perform hyperparameter scan\n",
    "experiment_hyperparams2 = experiment.xgboost_hyperparameter_scan(\n",
    "    param_grid, nodes_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_range = [4]\n",
    "learning_rate_range = [\n",
    "    0.1,\n",
    "    0.11,\n",
    "    0.12,\n",
    "    0.13,\n",
    "    0.14,\n",
    "    0.15,\n",
    "    0.16,\n",
    "    0.17,\n",
    "    0.18,\n",
    "    0.19,\n",
    "    0.2,\n",
    "    0.21,\n",
    "    0.22,\n",
    "    0.23,\n",
    "    0.24,\n",
    "    0.25,\n",
    "    0.26,\n",
    "    0.27,\n",
    "    0.28,\n",
    "    0.29,\n",
    "    0.30,\n",
    "    0.31,\n",
    "    0.32,\n",
    "    0.33,\n",
    "    0.34,\n",
    "    0.35,\n",
    "    0.36,\n",
    "    0.37,\n",
    "    0.38,\n",
    "    0.39,\n",
    "    0.40,\n",
    "    0.41,\n",
    "    0.42,\n",
    "    0.43,\n",
    "    0.44,\n",
    "    0.45,\n",
    "    0.46,\n",
    "    0.48,\n",
    "    0.5,\n",
    "    0.52,\n",
    "    0.53,\n",
    "    0.55,\n",
    "    0.57,\n",
    "    0.59,\n",
    "    0.6,\n",
    "    0.63,\n",
    "    0.65,\n",
    "    0.66,\n",
    "    0.67,\n",
    "    0.68,\n",
    "    0.69,\n",
    "    0.7,\n",
    "    0.71,\n",
    "    0.72,\n",
    "    0.73,\n",
    "    0.74,\n",
    "    0.75,\n",
    "    0.76,\n",
    "    0.77,\n",
    "    0.78,\n",
    "    0.79,\n",
    "    0.8,\n",
    "    0.81,\n",
    "    0.82,\n",
    "    0.83,\n",
    "    0.84,\n",
    "    0.85,\n",
    "]\n",
    "colsample_bytree_range = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "num_rounds_grid = [130]\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(\n",
    "    itertools.product(\n",
    "        max_depth_range, learning_rate_range, colsample_bytree_range, num_rounds_grid\n",
    "    )\n",
    ")\n",
    "\n",
    "# Construct the param_grid\n",
    "param_grid = [\n",
    "    {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"max_depth\": max_depth,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"n_estimators\": n_estimators,\n",
    "    }\n",
    "    for max_depth, learning_rate, colsample_bytree, n_estimators in param_combinations\n",
    "]\n",
    "\n",
    "\n",
    "# Perform hyperparameter scan\n",
    "experiment_hyperparams3 = experiment.xgboost_hyperparameter_scan(\n",
    "    param_grid, nodes_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_range = [4, 5]\n",
    "learning_rate_range = [0.74]\n",
    "colsample_bytree_range = [1.0]\n",
    "num_rounds_grid = [60, 65, 70, 75, 80, 85, 100, 130]\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(\n",
    "    itertools.product(\n",
    "        max_depth_range, learning_rate_range, colsample_bytree_range, num_rounds_grid\n",
    "    )\n",
    ")\n",
    "\n",
    "# Construct the param_grid\n",
    "param_grid = [\n",
    "    {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"max_depth\": max_depth,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"n_estimators\": n_estimators,\n",
    "    }\n",
    "    for max_depth, learning_rate, colsample_bytree, n_estimators in param_combinations\n",
    "]\n",
    "\n",
    "\n",
    "# Perform hyperparameter scan\n",
    "experiment_hyperparams4 = experiment.xgboost_hyperparameter_scan(\n",
    "    param_grid, nodes_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_range = [3, 4, 5, 6]\n",
    "learning_rate_range = [0.74]\n",
    "colsample_bytree_range = [1.0]\n",
    "num_rounds_grid = [130, 150, 200, 275, 300, 600]\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(\n",
    "    itertools.product(\n",
    "        max_depth_range, learning_rate_range, colsample_bytree_range, num_rounds_grid\n",
    "    )\n",
    ")\n",
    "\n",
    "# Construct the param_grid\n",
    "param_grid = [\n",
    "    {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"max_depth\": max_depth,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"n_estimators\": n_estimators,\n",
    "    }\n",
    "    for max_depth, learning_rate, colsample_bytree, n_estimators in param_combinations\n",
    "]\n",
    "\n",
    "\n",
    "# Perform hyperparameter scan\n",
    "experiment_hyperparams5 = experiment.xgboost_hyperparameter_scan(\n",
    "    param_grid, nodes_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train miniaturised model\n",
    "print(experiment.X_test.shape)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"max_depth\": 4,\n",
    "    \"learning_rate\": 0.74,\n",
    "    \"gamma\": 5000,  # min split loss\n",
    "    \"colsample_bytree\": 1.0,\n",
    "    \"eval_metric\": \"rmse\",\n",
    "}\n",
    "\n",
    "num_rounds = 130\n",
    "\n",
    "\n",
    "experiment.train_xgboost_model(params, num_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.count_nodes(experiment.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_e = experiment.testing_dataset.physics_object_type == \"electron\"\n",
    "mask_p = experiment.testing_dataset.physics_object_type == \"photon\"\n",
    "mask_pi0 = experiment.testing_dataset.physics_object_type == \"pi0\"\n",
    "mask_pi_char = experiment.testing_dataset.physics_object_type == \"pi_char\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_e, y_e, x_u_e, u_e = ed.plot_avg(\n",
    "    x_values=experiment.testing_dataset[\"et\"].values[mask_e],\n",
    "    y_values=(experiment.y_test - experiment.y_pred)[mask_e],\n",
    "    interval=2000,\n",
    "    xlabel=\"true et [MeV]\",\n",
    "    rms=True,\n",
    "    return_values=True,\n",
    "    ylabel=\"rms\",\n",
    "    return_x_u=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_p, y_p, x_u_p, u_p = ed.plot_avg(\n",
    "    x_values=experiment.testing_dataset[\"et\"].values[mask_p],\n",
    "    y_values=(experiment.y_test - experiment.y_pred)[mask_p],\n",
    "    interval=2000,\n",
    "    xlabel=\"true et [MeV]\",\n",
    "    rms=True,\n",
    "    ylabel=\"rms\",\n",
    "    return_values=True,\n",
    "    return_x_u=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pi0, y_pi0, x_u_pi0, u_pi0 = ed.plot_avg(\n",
    "    x_values=experiment.testing_dataset[\"et\"].values[mask_pi0],\n",
    "    y_values=(experiment.y_test - experiment.y_pred)[mask_pi0],\n",
    "    interval=2000,\n",
    "    xlabel=\"true et [MeV]\",\n",
    "    rms=True,\n",
    "    ylabel=\"rms\",\n",
    "    return_values=True,\n",
    "    return_x_u=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pi_char, y_pi_char, x_u_pi_char, u_pi_char = ed.plot_avg(\n",
    "    x_values=experiment.testing_dataset[\"et\"].values[mask_pi_char],\n",
    "    y_values=(experiment.y_test - experiment.y_pred)[mask_pi_char],\n",
    "    interval=2000,\n",
    "    xlabel=\"true et [MeV]\",\n",
    "    rms=True,\n",
    "    return_values=True,\n",
    "    ylabel=\"rms\",\n",
    "    return_x_u=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = experiment.y_test\n",
    "Y_pred = experiment.y_pred\n",
    "ed.plot_predictions(Y_test, Y_pred)\n",
    "\n",
    "ed.plot_errors(Y_test, Y_pred, xmax=350, xcut=350, binnum=100)\n",
    "\n",
    "ed.plot_corelation(Y_test, Y_pred, density=True, log_density=False, plot_line=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
